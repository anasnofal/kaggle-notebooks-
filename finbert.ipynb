{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":9097274,"sourceType":"datasetVersion","datasetId":5490281}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=false\n%env WANDB_DISABLED=true\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel, \n    AdamW, \n    AutoConfig, \n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n    AutoModelForSequenceClassification\n)\n\nfrom datasets import Dataset\n\nimport torch\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:35.384973Z","iopub.execute_input":"2025-06-05T15:55:35.385278Z","iopub.status.idle":"2025-06-05T15:55:51.915151Z","shell.execute_reply.started":"2025-06-05T15:55:35.385238Z","shell.execute_reply":"2025-06-05T15:55:51.914236Z"}},"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\nenv: WANDB_DISABLED=true\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class args:\n    model = 'ProsusAI/finbert'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:51.916167Z","iopub.execute_input":"2025-06-05T15:55:51.916820Z","iopub.status.idle":"2025-06-05T15:55:51.920359Z","shell.execute_reply.started":"2025-06-05T15:55:51.916784Z","shell.execute_reply":"2025-06-05T15:55:51.919522Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndf = pd.read_csv('/kaggle/input/news-sentiment-analysis/news.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:53.728368Z","iopub.execute_input":"2025-06-05T15:55:53.728718Z","iopub.status.idle":"2025-06-05T15:55:55.619789Z","shell.execute_reply.started":"2025-06-05T15:55:53.728692Z","shell.execute_reply":"2025-06-05T15:55:55.618855Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.rename(columns={'sentiment': 'labels', 'news': 'messages'}, inplace=True)\n\n# Set date as index\ndf['date'] = pd.to_datetime(df['date'])  # Convert to datetime if necessary\ndf.set_index('date', inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:55.621249Z","iopub.execute_input":"2025-06-05T15:55:55.621604Z","iopub.status.idle":"2025-06-05T15:55:55.686119Z","shell.execute_reply.started":"2025-06-05T15:55:55.621570Z","shell.execute_reply":"2025-06-05T15:55:55.685444Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"le = LabelEncoder()\ndf['labels'] = le.fit_transform(df['labels'])\ndf['labels'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:55.687346Z","iopub.execute_input":"2025-06-05T15:55:55.687759Z","iopub.status.idle":"2025-06-05T15:55:55.760134Z","shell.execute_reply.started":"2025-06-05T15:55:55.687734Z","shell.execute_reply":"2025-06-05T15:55:55.759520Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"labels\n0    217443\n1    210039\nName: count, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"sentences_lengths = np.array(list(map(len, df['messages'])))\nnp.max(sentences_lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:56.428097Z","iopub.execute_input":"2025-06-05T15:55:56.428381Z","iopub.status.idle":"2025-06-05T15:55:56.509416Z","shell.execute_reply.started":"2025-06-05T15:55:56.428359Z","shell.execute_reply":"2025-06-05T15:55:56.508481Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1586"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"MAX_LEN = int(np.ceil(np.percentile(sentences_lengths, 90)))\nMAX_LEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:57.339560Z","iopub.execute_input":"2025-06-05T15:55:57.339887Z","iopub.status.idle":"2025-06-05T15:55:57.351801Z","shell.execute_reply.started":"2025-06-05T15:55:57.339863Z","shell.execute_reply":"2025-06-05T15:55:57.351059Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"177"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"Now let's split the given data into train, test and validation\n\nWe will be using the train data to train the mode, the validation data to determiine the performance of the model and the test data to check how the model performs on unseen data\n\nWe will stratify on the `labels` so that the data remains balanced for train, test and validation data","metadata":{"execution":{"iopub.status.busy":"2022-03-21T08:54:02.661079Z","iopub.execute_input":"2022-03-21T08:54:02.661604Z","iopub.status.idle":"2022-03-21T08:54:02.674078Z","shell.execute_reply.started":"2022-03-21T08:54:02.661565Z","shell.execute_reply":"2022-03-21T08:54:02.673369Z"}}},{"cell_type":"code","source":"# X, y = df['messages'].values, df['labels'].values\n\n# # train : test = 0.9 : 0.1\n# xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, stratify=y)\n\n# # train : valid = 0.8 : 0.2\n# xtrain, xvalid, ytrain, yvalid = train_test_split(xtrain, ytrain, test_size=0.2, stratify=ytrain)\n\n# # train : valid : test = 0.72 : 0.18 : 0.10 (stratified on 'labels')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:59.684436Z","iopub.execute_input":"2025-06-05T15:55:59.684765Z","iopub.status.idle":"2025-06-05T15:55:59.688115Z","shell.execute_reply.started":"2025-06-05T15:55:59.684741Z","shell.execute_reply":"2025-06-05T15:55:59.687375Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df= df.sample(frac=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:55:59.900294Z","iopub.execute_input":"2025-06-05T15:55:59.900569Z","iopub.status.idle":"2025-06-05T15:55:59.934116Z","shell.execute_reply.started":"2025-06-05T15:55:59.900547Z","shell.execute_reply":"2025-06-05T15:55:59.933300Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df = df.sort_index()\n\ntotal_rows = len(df)\ntrain_end = int(total_rows * 0.72)\nvalid_end = train_end + int(total_rows * 0.18)\n\ntrain = df.iloc[:train_end]\nvalid = df.iloc[train_end:valid_end]\ntest = df.iloc[valid_end:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:00.072438Z","iopub.execute_input":"2025-06-05T15:56:00.072712Z","iopub.status.idle":"2025-06-05T15:56:00.080817Z","shell.execute_reply.started":"2025-06-05T15:56:00.072691Z","shell.execute_reply":"2025-06-05T15:56:00.080083Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Convert to NumPy arrays\nX_train = train.drop(columns=['labels']).to_numpy()  # Features for training\ny_train = train['labels'].to_numpy()                # Target labels for training\n\nX_valid = valid.drop(columns=['labels']).to_numpy()  # Features for validation\ny_valid = valid['labels'].to_numpy()                 # Target labels for validation\n\nX_test = test.drop(columns=['labels']).to_numpy()    # Features for testing\ny_test = test['labels'].to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:00.245770Z","iopub.execute_input":"2025-06-05T15:56:00.245987Z","iopub.status.idle":"2025-06-05T15:56:00.252528Z","shell.execute_reply.started":"2025-06-05T15:56:00.245968Z","shell.execute_reply":"2025-06-05T15:56:00.251718Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"X_train = [str(x) for x in X_train]\nX_valid = [str(x) for x in X_valid]\nX_test = [str(x) for x in X_test]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:01.788635Z","iopub.execute_input":"2025-06-05T15:56:01.788970Z","iopub.status.idle":"2025-06-05T15:56:01.803683Z","shell.execute_reply.started":"2025-06-05T15:56:01.788943Z","shell.execute_reply":"2025-06-05T15:56:01.802800Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_dataset_raw = Dataset.from_dict({'text':X_train, 'labels':y_train})\nvalid_dataset_raw = Dataset.from_dict({'text':X_valid, 'labels':y_valid})\npred_dataset_raw = Dataset.from_dict({'text': X_test})\npred_dataset_raw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:01.921002Z","iopub.execute_input":"2025-06-05T15:56:01.921207Z","iopub.status.idle":"2025-06-05T15:56:01.945239Z","shell.execute_reply.started":"2025-06-05T15:56:01.921190Z","shell.execute_reply":"2025-06-05T15:56:01.944416Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 44\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def tokenize_fn(examples):\n    return tokenizer(examples['text'], truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:03.234238Z","iopub.execute_input":"2025-06-05T15:56:03.234552Z","iopub.status.idle":"2025-06-05T15:56:03.238131Z","shell.execute_reply.started":"2025-06-05T15:56:03.234520Z","shell.execute_reply":"2025-06-05T15:56:03.237278Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(args.model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:03.437998Z","iopub.execute_input":"2025-06-05T15:56:03.438342Z","iopub.status.idle":"2025-06-05T15:56:04.311812Z","shell.execute_reply.started":"2025-06-05T15:56:03.438306Z","shell.execute_reply":"2025-06-05T15:56:04.311032Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2814524f0ad041078e9c22b19a61d654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9244c4f1be14b219129753bb3fad018"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da6ecc6e1054ff085ed1d23ea846248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd914f5e70174a76b0a7096d798c4ba1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"train_dataset_raw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:04.312692Z","iopub.execute_input":"2025-06-05T15:56:04.312967Z","iopub.status.idle":"2025-06-05T15:56:04.317701Z","shell.execute_reply.started":"2025-06-05T15:56:04.312945Z","shell.execute_reply":"2025-06-05T15:56:04.316935Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'labels'],\n    num_rows: 307\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"train_dataset = train_dataset_raw.map(tokenize_fn, batched=True)\nvalid_dataset = valid_dataset_raw.map(tokenize_fn, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:04.984970Z","iopub.execute_input":"2025-06-05T15:56:04.985252Z","iopub.status.idle":"2025-06-05T15:56:05.120055Z","shell.execute_reply.started":"2025-06-05T15:56:04.985228Z","shell.execute_reply":"2025-06-05T15:56:05.119123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/307 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cafd8bde8a8541cbafc721e7139d7c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/76 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76b060421dc5432a8a234984f6474bb8"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from torch.nn.init import xavier_uniform_\nmodel = AutoModelForSequenceClassification.from_pretrained(args.model,num_labels=2,ignore_mismatched_sizes=True)\n\n\n# Reinitialize the classification layer\nxavier_uniform_(model.classifier.weight)\nmodel.classifier.bias.data.zero_()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:05.186758Z","iopub.execute_input":"2025-06-05T15:56:05.187052Z","iopub.status.idle":"2025-06-05T15:56:08.930289Z","shell.execute_reply.started":"2025-06-05T15:56:05.187027Z","shell.execute_reply":"2025-06-05T15:56:08.929602Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669fdc5304cb4b7b9e0b6ac5741ee120"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([0., 0.])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\nimport optuna\nimport torch\nfrom transformers import Trainer, TrainingArguments, AdamW\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\nfrom transformers import get_scheduler\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    balanced_acc = balanced_accuracy_score(labels, predictions)\n    auc = roc_auc_score(labels, predictions, multi_class='ovr')  # Adjust for multi-class\n    f1 = f1_score(labels, predictions, average='weighted')\n    return {\n        \"balanced_accuracy\": balanced_acc,\n        \"auc\": auc,\n        \"f1\": f1\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:08.931221Z","iopub.execute_input":"2025-06-05T15:56:08.931523Z","iopub.status.idle":"2025-06-05T15:56:09.128079Z","shell.execute_reply.started":"2025-06-05T15:56:08.931472Z","shell.execute_reply":"2025-06-05T15:56:09.127402Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nimport optuna\nfrom optuna.pruners import MedianPruner\nfrom optuna.samplers import TPESampler\nimport numpy as np\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, f1_score\nimport torch\nfrom transformers.integrations import is_optuna_available\nimport joblib\n\n\n# Define a custom compute_metrics function that returns multiple metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Convert predictions to probabilities for AUC (for binary classification)\n    probs = torch.nn.functional.softmax(torch.tensor(pred.predictions), dim=-1).numpy()\n    \n    # Calculate metrics\n    balanced_acc = balanced_accuracy_score(labels, preds)\n    \n    # Use the probability of the positive class\n    auc = roc_auc_score(labels, probs[:, 1])\n    f1 = f1_score(labels, preds)\n    \n    return {\n        'balanced_accuracy': balanced_acc,\n        'auc': auc,\n        'f1': f1\n    }\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to optimize\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 0.005, 0.02)\n    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.05, 0.2)\n    num_epochs = trial.suggest_int(\"num_epochs\", 2, 8)\n    \n    # Optimizer selection\n    optimizer_name = trial.suggest_categorical(\"optimizer_name\", [ \"adamw_hf\",\"adafactor\", \"sgd\"])\n    \n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=f'kaggle/Finbert_Trial_{trial.number}/',\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size*2,\n        num_train_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        warmup_ratio=warmup_ratio,\n        do_eval=True,\n        do_train=True,\n        evaluation_strategy='epoch',\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model='eval_f1',  # Change this to your desired metric\n        greater_is_better=True,\n        optim=optimizer_name.lower(),\n        # ReduceLROnPlateau-like functionality\n        lr_scheduler_type=\"reduce_lr_on_plateau\",\n        logging_dir=f'kaggle/Finbert_Logs_{trial.number}/',\n        report_to=\"none\",  # Disable wandb, tensorboard etc. during hyperparameter search\n    )\n    \n    # Initialize the Trainer\n    trainer = Trainer(\n        model=model,  # Make sure this is already defined in your global scope\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    # Evaluate on validation set\n    eval_result = trainer.evaluate()\n    try:\n        joblib.dump(study, 'finbert_optuna_result.pkl')\n    except Exception as e:\n        print(\"There is a tiny error with saving. Moving on... \", e)\n        print(param)\n\n    return eval_result[\"eval_f1\"]\n\n\nfrom stopit import threading_timeoutable as timeoutable\n \n@timeoutable()\ndef start():\n    ### import warnings \n    import warnings \n    import optuna\n \n    warnings.filterwarnings(\"ignore\")\n \n    optuna.logging.set_verbosity(optuna.logging.ERROR)\n    global study\n \n    import os\n    if os.path.isfile('finbert_optuna_result.pkl'):\n        print(\"Found file !\")\n        study = joblib.load('finbert_optuna_result.pkl')\n        print(study.best_trial.value)\n    else:\n        sampler = optuna.samplers.TPESampler(multivariate=True, warn_independent_sampling=False)\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n\n        study = optuna.create_study(direction='maximize', load_if_exists=True,sampler=sampler,pruner=pruner)\n \n    study.optimize(objective, n_trials=1_00, gc_after_trial=True,\n                  show_progress_bar=True, n_jobs=1, )\n    joblib.dump(study, 'finbert_optuna_result.pkl')\n\n\n# # Create the Optuna study\n# sampler = TPESampler(seed=42)  # For reproducibility\n# pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n\n# study = optuna.create_study(\n#     study_name=\"finbert_optimization\",\n#     direction=\"maximize\",  # We want to maximize our metrics\n#     sampler=sampler,\n#     pruner=pruner\n# )\n\n# # Run the optimization\n# n_trials = 8\n# study.optimize(objective, n_trials=n_trials)\n\n \ntry:\n    start(timeout=1000) #42_500\nexcept Exception as e:\n    print(\"Hoba, oshibka !: \", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:56:09.775687Z","iopub.execute_input":"2025-06-05T15:56:09.775978Z","iopub.status.idle":"2025-06-05T15:58:12.733928Z","shell.execute_reply.started":"2025-06-05T15:56:09.775954Z","shell.execute_reply":"2025-06-05T15:58:12.732832Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f290add404b6480d8a444d812ecd2d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 00:34, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Balanced Accuracy</th>\n      <th>Auc</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.693713</td>\n      <td>0.653500</td>\n      <td>0.747748</td>\n      <td>0.711111</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.739242</td>\n      <td>0.642065</td>\n      <td>0.767152</td>\n      <td>0.682353</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.087393</td>\n      <td>0.712405</td>\n      <td>0.792793</td>\n      <td>0.694444</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.114897</td>\n      <td>0.712405</td>\n      <td>0.854470</td>\n      <td>0.694444</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.225330</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.741991</td>\n      <td>0.713791</td>\n      <td>0.871795</td>\n      <td>0.676471</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:34, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Balanced Accuracy</th>\n      <th>Auc</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.225355</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.225344</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.225369</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.225350</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.225341</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.225369</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.225448</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [35/35 00:25, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Balanced Accuracy</th>\n      <th>Auc</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.225367</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.225375</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.225379</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>1.225392</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>1.225457</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.225466</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.225481</td>\n      <td>0.777893</td>\n      <td>0.885655</td>\n      <td>0.767123</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:13, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Balanced Accuracy</th>\n      <th>Auc</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.620172</td>\n      <td>0.752252</td>\n      <td>0.896743</td>\n      <td>0.732394</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.766687</td>\n      <td>0.752252</td>\n      <td>0.896050</td>\n      <td>0.732394</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/30 00:05 < 00:07, 2.17 it/s, Epoch 1.20/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Balanced Accuracy</th>\n      <th>Auc</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.721319</td>\n      <td>0.739432</td>\n      <td>0.898822</td>\n      <td>0.714286</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-f501a177100d>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#42_500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hoba, oshibka !: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stopit/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0;31m# ``result`` may not be assigned below in case of timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-f501a177100d>\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     study.optimize(objective, n_trials=1_00, gc_after_trial=True,\n\u001b[0m\u001b[1;32m    118\u001b[0m                   show_progress_bar=True, n_jobs=1, )\n\u001b[1;32m    119\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'finbert_optuna_result.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-f501a177100d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2282\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m                 ):\n\u001b[1;32m   2286\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"!rm -rf /kaggle/working/Finbert_Trial_4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T15:59:22.454060Z","iopub.execute_input":"2025-06-05T15:59:22.454362Z","iopub.status.idle":"2025-06-05T15:59:22.700384Z","shell.execute_reply.started":"2025-06-05T15:59:22.454340Z","shell.execute_reply":"2025-06-05T15:59:22.699431Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Print the results\n\nstudy = joblib.load('finbert_optuna_result.pkl')\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Value: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\n# Train the final model with the best parameters\nbest_batch_size = trial.params[\"batch_size\"]\nbest_lr = trial.params[\"learning_rate\"]\nbest_weight_decay = trial.params[\"weight_decay\"]\nbest_warmup_ratio = trial.params[\"warmup_ratio\"]\nbest_num_epochs = trial.params[\"num_epochs\"]\nbest_optimizer = trial.params[\"optimizer\"]\n\nfinal_training_args = TrainingArguments(\n    output_dir='./Finbert_Final/',\n    per_device_train_batch_size=best_batch_size,\n    per_device_eval_batch_size=best_batch_size*2,\n    num_train_epochs=best_num_epochs,\n    learning_rate=best_lr,\n    weight_decay=best_weight_decay,\n    warmup_ratio=best_warmup_ratio,\n    do_eval=True,\n    do_train=True,\n    evaluation_strategy='epoch',\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    optim=best_optimizer.lower(),\n    lr_scheduler_type=\"reduce_lr_on_plateau\",\n)\n\nfinal_trainer = Trainer(\n    model=model,\n    args=final_training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train the final model\nprint(\"Training final model with best parameters...\")\nfinal_trainer.train()\n\n# Evaluate the final model\nfinal_metrics = final_trainer.evaluate()\nprint(f\"Final model metrics: {final_metrics}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_args = TrainingArguments(\n    './Finbert Trained/',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=2*16,\n    num_train_epochs=5,\n    learning_rate=1e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,    \n    do_eval=True,\n    do_train=True,\n    do_predict=True,\n    eval_strategy='epoch',\n    save_strategy=\"no\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    train_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model('finbert_finetuned.bin')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_dataset = pred_dataset_raw.map(tokenize_fn, batched=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(pred_dataset[0]['input_ids'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = trainer.predict(\n    test_dataset=pred_dataset,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [np.argmax(x) for x in output.predictions]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, preds)\n\n# Define class labels\nlabels = [0, 1]\n\n# Create a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=labels, yticklabels=labels)\n\n# Add labels and title\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix Heatmap')\n\n# Display the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy_score(y_test, preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, preds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"news = pd.read_csv('/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv',names= ['Sentiment','Sentence'],encoding='ISO-8859-1')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"news","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test, Y_test = news['Sentence'].values, news['Sentiment'].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le = LabelEncoder()\nY_test = le.fit_transform(Y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(Y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset_raw = Dataset.from_dict({'text': x_test})\ntest_dataset_raw","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = test_dataset_raw.map(tokenize_fn, batched=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = trainer.predict(\n    test_dataset=test_dataset,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = [np.argmax(x) for x in output.predictions]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform(x):\n    if x == 2:\n        return 1\n    elif x == 1:\n        return 1\n    else:  # x == 0\n        return 0\n        \nvectorized_transform = np.vectorize(transform)\nY_test = vectorized_transform(Y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(Y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(Y_test, preds)\n\n# Define class labels\nlabels = [0, 1]\n\n# Create a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=labels, yticklabels=labels)\n\n# Add labels and title\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix Heatmap')\n\n# Display the plot\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy_score(Y_test, preds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(Y_test, preds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}